---
title: "Homework 2"
author: "BUS 32100"
date: "Section 01 Due 11:59PM Oct 17 2019  \n Section 85 Due 11:59PM Oct 18 2019 "
output: 
  tufte::tufte_html:
    tufte_variant: "envisioned"
    highlight: pygments
link-citations: yes
---

```{r include=FALSE}
if (!require("tufte")) install.packages("tufte", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(tufte)
library(knitr)
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  digits = 2
  )
knitr::opts_chunk$set(eval = T)
```

# Instructions:
It is recommended that you collaborate with your classmates on this assignment. However, you must type **your own** code and submit your source file in either`.html` or `.Rmd` format on Canvas. 

In some cases, a **incomplete code** is given to guide you in the right direction. However, you will need to fill in the blanks in order to run the code block. Make sure to fill **all the blanks**, or comment them out, before you knit your R markdown file. Otherwise, it will return knitting errors. Challenge questions are for practise and extra credits. If you choose not to answer them, remove the `r` code block to avoid error messages. 

# 1. Data wrangling and visualization with college data

We will explore data on college majors and earnings, specifically the data behind the FiveThirtyEight story ["The Economic Guide To Picking A College Major"](https://fivethirtyeight.com/features/the-economic-guide-to-picking-a-college-major/).

This week we will use the `read_csv` function to read in our csv file:

```{marginfigure}
We read it in with the `read_csv` function, and save the result as a new data frame called `college_recent_grads`. Because `read_csv` is a function from `tidyverse`, this new data frame will be a tidy data frame.
```

```{r load-data}
college_recent_grads <- read_csv(
  "https://raw.githubusercontent.com/BUSN32100/data_files/master/recent-grads.csv"
  )
```

`college_recent_grads` is a tidy **data frame**, with each row 
representing an **observation** and each column representing a **variable**.

To view the data, you can take a quick peek at your data frame and view its dimensions with the `glimpse` function.

```{r glimpse}
glimpse(college_recent_grads)
```

The description of the variables, i.e. the **codebook**, is given below.

| Header                        |  Description
|:----------------|:--------------------------------
|`rank`                         | Rank by median earnings
|`major_code`                   | Major code, FO1DP in ACS PUMS
|`major`                        | Major description
|`major_category`               | Category of major from Carnevale et al
|`total`                        | Total number of people with major
|`sample_size`                  | Sample size (unweighted) of full-time, year-round ONLY (used for earnings)
|`men`                          | Male graduates
|`women`                        | Female graduates
|`sharewomen`                   | Women as share of total
|`employed`                     | Number employed (ESR == 1 or 2)
|`employed_full_time`           | Employed 35 hours or more
|`employed_part_time`           | Employed less than 35 hours
|`employed_full_time_yearround` | Employed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP >= 35)
|`unemployed`                   | Number unemployed (ESR == 3)
|`unemployment_rate`            | Unemployed / (Unemployed + Employed)
|`median`                       | Median earnings of full-time, year-round workers
|`p25th`                        | 25th percentile of earnigns
|`p75th`                        | 75th percentile of earnings
|`college_jobs`                 | Number with job requiring a college degree
|`non_college_jobs`             | Number with job not requiring a college degree
|`low_wage_jobs`                | Number in low-wage service jobs


## Which major has the lowest unemployment rate?

In order to answer this question all we need to do is sort the data. We use the
`arrange` function to do this, and sort it by the `unemployment_rate` variable. 
By default `arrange` sorts in ascending order, which is what we want here -- 
we're interested in the major with the *lowest* unemployment rate.

```{r lowest-unemp}
#college_recent_grads %>%
#  arrange(unemployment_rate) %>% 

```

This gives us what we wanted, but not in an ideal form. First, the name of 
the major barely fits on the page. Second, some of the variables are not 
that useful (e.g. `major_code`, `major_category`) and some we might want 
front and center are not easily viewed (e.g. `unemployment_rate`).

We can use the `select` function to choose which variables to display, and 
in which order:

```{r lowest-unemp-select}
college_recent_grads %>%
  arrange(unemployment_rate) %>%
  select(rank, major, unemployment_rate)
```

Ok, this is looking better, but do we really need all those decimal places in the 
unemployment variable? Not really!

- **1a.** Round `unemployment_rate`: We create a new variable with the `mutate` function. 
In this case, we're overwriting the existing `unemployment_rate` variable, by `round`ing it to `4` decimal places. Incomplete code is given below to guide you in the right direction, however you will need to fill in the blanks.

```{r}
college_recent_grads %>%
  arrange(unemployment_rate) %>%
  select(rank, major, unemployment_rate) %>%
  mutate(unemployment_rate = round(unemployment_rate, 4))
```


## Which major has the highest percentage of women?

To answer such a question we need to arrange the data in descending order. For 
example, if earlier we were interested in the major with the highest unemployment 
rate, we would use the following:

```{marginfigure}
The `desc` function specifies that we want `unemployment_rate` in descending order.
```

```{r}
#1.b)
college_recent_grads %>%
  arrange(desc(unemployment_rate)) %>%
  select(rank, major, unemployment_rate)
#The code below will list the majors with the most women in descending porder
college_recent_grads %>%
  arrange(desc(women)) %>%
  select(major,women,men)
#Lots of people in psychology and less in geological engineeriing!
college_recent_grads%>% 
  mutate(total = men + women, pct_women = round((women / total)*100, 2)) %>% 
  select(major, pct_women, total) %>% 
  arrange(desc(pct_women)) %>% 
  head(3)
```

- **1b.** Using what you've learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding `head(3)` at the end of the pipeline.
```{r}

```

## How do the distributions of median income compare across major categories?

```{marginfigure}
A percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: [Wikipedia](https://en.wikipedia.org/wiki/Percentile)
```

There are three types of incomes reported in this data frame: `p25th`, `median`,  and `p75th`. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.

The question we want to answer "How do the distributions of median income compare across major categories?". We need to do a few things to answer this question: First, we need to group the data by `major_category`. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.

 - **1c.**Let's start simple and take a look at the distribution of all median incomes using `geom_histogram`, without considering the major categories.

```{r}
#1.c)
ggplot(data = college_recent_grads, mapping = aes(x = median)) +
  geom_histogram()

```

- **1e.** Try binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice.
```{r}
#1.e)
ggplot(data = college_recent_grads, mapping = aes(x = median)) +
  geom_histogram(binwidth = 1000)
#I like 1000 more because you get more detail. with 5000 you only get a rough idea of the distribution
```

We can also calculate summary statistics for this distribution using the 
`summarise` function:

```{r}
college_recent_grads %>%
  summarise(min = min(median), max = max(median),
            mean = mean(median), med = median(median),
            sd = sd(median), 
            q1 = quantile(median, probs = 0.25),
            q3 = quantile(median, probs = 0.75))
#1.f)
#I think the median is the best in describing the distribution 
#because the median of 36000 is where, roughly,  the "center of gravity" of the data is. 


```

- **1f.** Based on the shape of the histogram you created in the previous 1e, determine which of these summary statistics above (min, max, mean, med, sd, q1, q3)  is useful for describing the distribution. Write up your description and include the summary statistic output as well.


- **1g.** Next, we facet the plot by major category. Plot the distribution of `median` income using a histogram, faceted by `major_category`. Use the `binwidth` you chose in 1e.
```{r}
#1.g)
ggplot(data = college_recent_grads, mapping = aes(x = median)) +
  geom_histogram(binwidth = 1000) +
  facet_wrap( ~ major_category, ncol = 4)
```


- **1h.** Use `` to find out which major has the highest median income? lowest? Which major has the `med` median income? 

```{r}
#I had to comment out my code below because I had knitting issues. 

#college_recent_grads %>%
#  print(rank==max())
#Looks like you can get the top paying majors without the  function. 
#college_recent_grads %>%
#  (rank==max(1)) %>% 
#  glimpse()
#Petroleum Engineering has the highest median earnings at $110,000! 
#college_recent_grads %>%
# (rank==max(173)) %>% 
#  glimpse()
#The lowest paying is Library Science lol. median pay $22k! 
#college_recent_grads %>%
#  (rank==max(87)) %>% 
#  glimpse()
#The median pay major is "Human Resources And Personnel Management" at $36k. 

```

- **1i.** Which major category is the most popular in this sample? To answer this question we use a new function called `count`, which first groups the data , then counts the number of observations in each category and store the counts into a column named `n`. Add to the pipeline appropriately to arrange the results so that the major with the highest observations is on top.

```{r}
college_recent_grads %>%
  count(major_category) %>% 
  arrange(desc(n))
#The most popular major category is engineering, followed by education. 

```

## What types of majors do women tend to major in?

First, let's create a new vector called `stem_categories` that lists the major categories that are considered STEM fields.

```{r}
stem_categories <- c("Biology & Life Science",
                     "Computers & Mathematics",
                     "Engineering",
                     "Physical Sciences")
```

Then, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.

```{r}
college_recent_grads <- college_recent_grads %>%
  mutate(major_type = ifelse(major_category %in% stem_categories, "stem", "not stem"))
```

Let's unpack this: with `mutate` we create a new variable called `major_type`, which is defined as `"stem"` if the `major_category` is in the nector called `stem_categories` we created earlier, and as `"not stem"` otherwise.


- **1j.** Create a scatterplot of median income vs. proportion of 
women in that major, colored by whether the major is in a STEM field or not. Describe the association between these three variables.

```{r}
college_recent_grads %>% 
  ggplot(aes(x = women / total, y = median)) +  geom_point(aes(color=major_category %in% stem_categories))
#Looks like there is an inverse relationship between percentage 
#of women in a major and mnedian salary. 
#Also non stem fields have a higher percentage of women. 
#Stem fields result in the salary. 
```


- **1k.**. We can use the logical operators to also `` our data for STEM majors whose median earnings is less than median for all majors's median earnings, which we found to be $36,000 earlier. Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major and should be sorted such that the major with the lowest median earning is on top.

```{r}
brokestems<-college_recent_grads %>%
  filter(
    major_type == "stem",
    median < 36000
  )
brokenstems2 <- brokestems[,c("major","p25th", "median","p75th")]
print(brokenstems2)

```



# 2. Modeling the burritos of San Diego
First, you can load the data using the following.

```{r data}
burrito <- read_csv('https://raw.githubusercontent.com/BUSN32100/data_files/master/burrito.csv')
```


## data wrangling and visualization


- **2a.**  Create a new variable called `core_avg` that is the average scores of the core dimensions of a burrito, **except `Cost`**. Add this new variable to the `burrito` data frame. Do this in one pipe, using the `rowwise` function. Incomplete code is given below to guide you in the right direction, however you will need to fill in the blanks.

```{marginfigure}
The `rowwise` function is useful for applying mathematical operations to each row.

```

```{marginfigure}
Core dimensions of a burrito:
2. Tortilla quality
3. Temperature
4. Meat quality
5. Non-meat filling quality
6. Meat to filling ratio
7. Uniformity 
8. Salsa quality
9. Wrap integrity
```

```{r}
core_avg <- burrito %>%
  rowwise() %>%
  mutate(core_avg = mean( c(Tortilla, Temp, Meat, Fillings, MeatToFilling, Uniformity,Salsa,Wrap) )) %>%
  ungroup()

print(core_avg)
```


Note that we end the pipeline with `ungroup()` to remove the effect of the `rowwise` function from earlier in the pipeline. The `rowwise` function works a lot like `group_by`(we will talk about this next week), except it groups the data frame one row at a time so that any operations applied to the data frame is done once per each row. This is helpful for finding the mean core dimension ratings *for each row*. However in the remainder of the analysis we don't want to, say, calculate summary statistics for each row, or fit a model for each row. Hence we need to undo the effect of `rowwise`, which we can do with `ungroup`.


- **2b.**  Visualize the distribution of `overall`. Is the distribution skewed? What does that tell you about how reviewer rate burritos? Is this what you expected to see? Why, or why not? Include any summary statistics and visualizations you use in your response.

```{r}
hist(burrito$overall)
#Looks like usually the overall score is between 3.5 and 4. 
#The distribution is skewed slightly to the higher side. 
#This is pretty close to what I expected to see because most 
#burritos will be average and a small portion will be in the tails. 
#Overall is very close to normal distribution, 
#which is the distribution I expected given the situation. 

```


- **2c.**  Visualize and describe the relationship between `overall` and the new variable you created, `core_avg`.
    
```{marginfigure}
**Hint:** See the help page for the function at http://ggplot2.tidyverse.org/reference/index.html.
```

```{r}
burrito %>% 
  ggplot(aes(x = core_avg$core_avg, y = overall)) +  geom_point()
```

    
- **2d.**  Replot the scatterplot from **2c.**, but this time use  `geom_jitter()`. What does "jitter" mean? What was misleading about the initial scatterplot?
```{r}
burrito %>% 
  ggplot(aes(x = core_avg$core_avg, y = overall)) +  geom_jitter()
#I read in the textbook that jitter adds some random variation to the data. 
#This may sound like a wierd thing you would want to add but with lots of data,
#apparently adding noise makes the signal you get from all 
#the data points easier to discern as it helps you grasp 
#where the density of observations is high.
```

## Linear regression with a numerical predictor

```{marginfigure}
Linear model is in the form $\hat{y} = b_0 + b_1 x$.
```

- **2e.**  Let's see if the apparent trend in the plot is something more than natural variation. Fit a linear model called `m_core` to predict average `overal` ratings by average core dimensions (`core_avg`). Based on the regression output, write the linear model.
```{r}
scatter.smooth(x=core_avg$core_avg, y=core_avg$overall, main="Core Average ~ Overall") 
m_core<- lm(core_avg$overall ~ core_avg$core_avg, data=burrito)
print(m_core)
#the linear regression has equation y=1.042x-0.144
summary(m_core)

```

    
- **2f.**  Replot your visualization from **2c.**, and add the regression line to this plot in orange color. Turn off the shading for the uncertainty of the line.
```{marginfigure}
**Hint:** use argument `method='lm'` to plot the linear fit, use `se=False` to turn off the uncertainty shading
```

```{r}
#scatter<-scatter.smooth() 

ggplot(burrito, aes(x=core_avg$core_avg, y=core_avg$overall, main="Core Average ~ Overall")) + 
  geom_point(shape=18, color="magenta")+
  geom_smooth(method=lm,  linetype="dashed",
            color="orange", fill="transparent")

```

- **2g.**  Interpret the slope of the linear model in context of the data.

- **2h.**  Interpret the intercept of the linear model in context of the data. Comment on whether  or not the intercept makes sense in this context.
    


## Linear regression with a categorical predictor

- **2i.**  Fit a new linear model called `m_rec` to predict average `overall` ratings based on `Rec` whether the reviewer recomends it or not. Based on the regression output, write the linear model and interpret the slope and intercept in context of the data.
```{r}
#2g) The slope of the linear model is positive. 
#This shows that there is a positive relationship between core_avg and overall. 

#2h) The fact that the y intercept is basically zero makes sense. 
#It means that even if youre core average is zero your 
#overall score will also be basically zero. It makes sense 
#since core_avg and overall are very similar concepts. 


#2i)
#If recommended: 
m_recyes<- lm(core_avg$overall ~ core_avg$Rec=="Yes", data=burrito) 
print(m_recyes)
#              (Intercept)  core_avg$Rec == "Yes"TRUE  
#                     2.87                       1.03  
#If not recommended:" 
m_recno<- lm(core_avg$overall ~ core_avg$Rec!="Yes", data=burrito) 
print(m_recno)
#(Intercept)  core_avg$Rec == "No"TRUE  
#   3.90                     -1.03  
#This shows that a recommended burrito will have a higher 
#score on average than a non-recommended burrito. 
#For any given value of x, the regression formula is such 
#that the former will be higher than the latter. 

ggplot(burrito, aes(x=core_avg$Rec, y=core_avg$overall, main="Core Average ~ Overall")) + 
  geom_point(color="magenta")+
  geom_smooth(method=lm,  linetype="dashed",
            color="orange", fill="orange")
#You can see by the plot that recommended burritos have higher overall scores. 
                 

```

    
- **2j.** What is the equation of the line corresponding to burritos that are recommended ? What is it for non-recommended burritos?

- **Challenge** Create a new variable called `CaliBurrito` that labels `Burrito` has string `'california'` in it  as "Yes"  and all other burrito types as "No".

```{r}

#Challenge 1    
CaliBurrito <- burrito %>%
  mutate(CaliBurrito = ifelse(Burrito=="california","Yes","No"))
print(CaliBurrito)

```

- **Challenge** Fit a new linear model called `m_cali` to predict average ratings `overall` based on `CaliBurrito`, `core_avg` and their interaction. This is the new variable you created in 12. Based on the regression output, write the linear model and interpret the coefficients in context of the data. Also determine and interpret the adjusted $R^2$ of the model.

```{r}

str(CaliBurrito)


multipleregression <- lm(CaliBurrito$overall ~ core_avg$core_avg + CaliBurrito$CaliBurrito)
print(multipleregression)

#Coefficients:
#               (Intercept)           core_avg$core_avg  
#                    -0.138                       1.031  
#CaliBurrito$CaliBurritoYes  
#                     0.104  

#I finally got a multiple regression function! 
#The columns i needed were disorganized and in different data 
#frames so for each column I had to specify the correct data 
#frame to take the column from. When I did this I was able to 
#get a regression. Looks like for each core-avg point your 
#overall score goes up by 1.031 but being a california burrito 
#only helps a little bit (overall score goes up by .104 if it's a caliburrito)

```
